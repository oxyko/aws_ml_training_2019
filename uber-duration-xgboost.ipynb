{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker XGBoost model applied to NYC Uber data\n",
    "\n",
    "Our SageMaker XGBoost regression model predicts trip duration based on feature vector that includes source zone, destination zone, and month, day and hour for the pickup time. \n",
    "\n",
    "The first step in using SageMaker is to create a SageMaker execution role that contains permissions used by SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "# Create SageMaker role \n",
    "role = get_execution_role()\n",
    "\n",
    "# get the url to the container image for using linear-learner\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "xgboost_image = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "print(xgboost_image)\n",
    "\n",
    "# destination bucket to upload protobuf recordIO files\n",
    "dest_bucket='aws-ajayvohra-nyc-tlc-sagemaker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data from CSV to protobuf recordIO Format\n",
    "\n",
    "We have multiple CSV files available in S3 bucket. Each CSV file contains numeric columns for encoded origin zone, encoded destination zone, month, day, hour, trip distance in miles and trip duration in seconds. \n",
    "\n",
    "We will download  multiple CSV files available in the source S3 bucket, convert each file to [protobuf recordIO](https://mxnet.incubator.apache.org/versions/master/architecture/note_data_loading.html#data-format) data format and upload the new files to the destination S3 bucket.\n",
    "\n",
    "During this conversion process, we will also split the csv files into training, validation and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import csv\n",
    "\n",
    "# source bucket with CSV files\n",
    "source_bucket='aws-ajayvohra-nyc-tlc-glue'\n",
    "source_prefix='uber'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response=s3.list_objects_v2(Bucket=source_bucket, Prefix=source_prefix)\n",
    "contents=response['Contents']\n",
    "count=len(contents)\n",
    "\n",
    "sbucket = boto3.resource('s3').Bucket(source_bucket)\n",
    "ntrain=int(count*0.8)\n",
    "nval = int(count*0.2)\n",
    "ntest = count - ntrain - nval\n",
    "\n",
    "def transform_upload(start, end, name):\n",
    "    data=tempfile.NamedTemporaryFile(mode='w+t', suffix='.csv', prefix='data-', delete=True)\n",
    "    data_writer = open(data.name, 'w+t')\n",
    "    \n",
    "    for i in range(start, end, 1):\n",
    "        item=contents[i]\n",
    "        key =item['Key']    \n",
    "        print(\"tranforming: \"+key)\n",
    "        with tempfile.NamedTemporaryFile(mode='w+b', suffix='.csv', prefix='data-', delete=True) as csv_file:\n",
    "            sbucket.download_fileobj(key, csv_file)\n",
    "            csv_file2=open(csv_file.name, 'rt')\n",
    "            \n",
    "            csv_writer=csv.writer(data_writer, delimiter=',')\n",
    "            csv_reader = csv.reader(csv_file2, delimiter=',')\n",
    "            header=True\n",
    "            print(\"reading: \"+csv_file.name)\n",
    "            for row in csv_reader:\n",
    "                if not header:\n",
    "                    new_row=[row[-1]]+row[0:5]\n",
    "                    csv_writer.writerow(new_row)\n",
    "                else:\n",
    "                    header=False\n",
    "            csv_file2.close()\n",
    "            csv_file.close()\n",
    "    data_writer.close()\n",
    "    \n",
    "    with open(data.name, 'rb') as data_reader:\n",
    "        print(f'upload {name} data file')\n",
    "        s3.upload_fileobj(data_reader, dest_bucket, f'csv/{name}/data.csv')\n",
    "        data_reader.close()\n",
    "    \n",
    "    data.close()\n",
    "\n",
    "transform_upload(0, ntrain, 'train')\n",
    "transform_upload(ntrain, ntrain+nval, 'validation')\n",
    "transform_upload(ntrain+nval, count, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data input channels\n",
    "\n",
    "We will create train, validaiton and test input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import s3_input\n",
    "\n",
    "s3_train = s3_input(s3_data=f's3://{dest_bucket}/csv/train', content_type='csv')\n",
    "s3_validation = s3_input(s3_data=f's3://{dest_bucket}/csv/validation', content_type='csv')\n",
    "s3_test = s3_input(s3_data=f's3://{dest_bucket}/csv/test', content_type='csv')\n",
    "\n",
    "output_path=f's3://{dest_bucket}/output/xgboost/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SageMaker Linear Learner Estimator\n",
    "\n",
    "SageMaker Estimator class defines the SageMaker job for training Linear Learner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "\n",
    "linear_learner = Estimator(image_name=xgboost_image,\n",
    "                            role=role, \n",
    "                            train_instance_count=1, \n",
    "                            train_instance_type='ml.m5.12xlarge',\n",
    "                            output_path=output_path,\n",
    "                            sagemaker_session=sagemaker_session)\n",
    "linear_learner.set_hyperparameters(num_round=20, objective='reg:linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_learner.fit(inputs={'train': s3_train, 'validation': s3_validation, 'test': s3_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
